{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96312069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55407a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Tim Kaine doesn’t want a border at all. He wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The deficit ... is coming down, and it’s comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Migrant mother and ‘crying girl’ on Time cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Fact: Over 90,000 kids were detained under Ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>\"$1 billion—that’s how much Bruce Rauner has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    veracity                                          statement\n",
       "3          0  \"Tim Kaine doesn’t want a border at all. He wa...\n",
       "5          0  \"The deficit ... is coming down, and it’s comi...\n",
       "20         1  \"Migrant mother and ‘crying girl’ on Time cove...\n",
       "23         0  \"Fact: Over 90,000 kids were detained under Ob...\n",
       "24         0  \"$1 billion—that’s how much Bruce Rauner has w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv to df\n",
    "path = os.path.abspath('politifact_balanced_data.csv')\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# specify rows of importance\n",
    "df = df[[\"veracity\", \"statement\"]]\n",
    "\n",
    "# See sample of data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af281402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2165 entries, 3 to 11183\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   2165 non-null   int64 \n",
      " 1   statement  2165 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 50.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299395e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "df_validation = df.drop(train.index)\n",
    "validation=df_validation.sample(frac=0.6,random_state=200)\n",
    "test = df_validation.drop(validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996053c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1732 entries, 698 to 4956\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   1732 non-null   int64 \n",
      " 1   statement  1732 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 40.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 173 entries, 70 to 11183\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   173 non-null    int64 \n",
      " 1   statement  173 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5639b",
   "metadata": {},
   "source": [
    "# Transforming the statements into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab8c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_text = train['statement'].to_numpy()\n",
    "tok = Tokenizer(oov_token='<unk>')\n",
    "tok.fit_on_texts(train_text)\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "train_labels = train['veracity'].to_numpy().flatten()\n",
    "\n",
    "\n",
    "# CONVERT TO TF DATASETS\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 7\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# PREFETCH\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09ac399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55/55 [==============================] - 10s 76ms/step - loss: 0.6836 - accuracy: 0.5508\n",
      "Epoch 2/5\n",
      "55/55 [==============================] - 4s 76ms/step - loss: 0.4578 - accuracy: 0.7846\n",
      "Epoch 3/5\n",
      "55/55 [==============================] - 4s 75ms/step - loss: 0.1819 - accuracy: 0.9342\n",
      "Epoch 4/5\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 0.0526 - accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 0.0310 - accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc7f7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6836201548576355,\n",
       "  0.4578196406364441,\n",
       "  0.18188466131687164,\n",
       "  0.052647653967142105,\n",
       "  0.031013986095786095],\n",
       " 'accuracy': [0.550808310508728,\n",
       "  0.7846420407295227,\n",
       "  0.9341801404953003,\n",
       "  0.9815242290496826,\n",
       "  0.9901847839355469]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c132c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 24ms/step - loss: 1.9571 - accuracy: 0.5723\n"
     ]
    }
   ],
   "source": [
    "test_text = test['statement'].to_numpy()\n",
    "test_seqs = tok.texts_to_sequences(test_text)\n",
    "test_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')\n",
    "\n",
    "test_labels = test['veracity'].to_numpy().flatten()\n",
    "\n",
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9063f4d",
   "metadata": {},
   "source": [
    "# Avoid Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ee292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.65,random_state=200) #random state is a seed value\n",
    "df_validation = df.drop(train.index)\n",
    "validation=df_validation.sample(frac=0.6,random_state=200)\n",
    "test = df_validation.drop(validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960beec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_text = train['statement'].to_numpy()\n",
    "tok = Tokenizer(oov_token='<unk>')\n",
    "tok.fit_on_texts(train_text)\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "train_labels = train['veracity'].to_numpy().flatten()\n",
    "\n",
    "valid_text = validation['statement'].to_numpy()\n",
    "valid_seqs = tok.texts_to_sequences(valid_text)\n",
    "valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')\n",
    "\n",
    "valid_labels = validation['veracity'].to_numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT TO TF DATASETS\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 7\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# PREFETCH\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ffda02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 8s 387ms/step - loss: 0.6941 - accuracy: 0.4554 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.6927 - accuracy: 0.5134 - val_loss: 0.6894 - val_accuracy: 0.5625\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.6859 - accuracy: 0.5491 - val_loss: 0.6872 - val_accuracy: 0.5625\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.6969 - accuracy: 0.4821 - val_loss: 0.6873 - val_accuracy: 0.4688\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.6806 - accuracy: 0.5893 - val_loss: 0.6801 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82aed414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 8s 395ms/step - loss: 4.2884 - accuracy: 0.4777 - val_loss: 3.9875 - val_accuracy: 0.5312\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 3.7639 - accuracy: 0.5179 - val_loss: 3.4686 - val_accuracy: 0.5312\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 3.2544 - accuracy: 0.5536 - val_loss: 2.9857 - val_accuracy: 0.5312\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 2.8071 - accuracy: 0.4643 - val_loss: 2.5657 - val_accuracy: 0.5625\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 2.4063 - accuracy: 0.5357 - val_loss: 2.2075 - val_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.GRU(128, kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b93e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.5065 - accuracy: 0.5963\n",
      "Epoch 2/5\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 0.7355 - accuracy: 0.7562\n",
      "Epoch 3/5\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 0.3813 - accuracy: 0.8678\n",
      "Epoch 4/5\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 0.2279 - accuracy: 0.9318\n",
      "Epoch 5/5\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 0.1177 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit( train_ds,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e9df473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.5064618587493896,\n",
       "  0.7355286478996277,\n",
       "  0.38134974241256714,\n",
       "  0.22788342833518982,\n",
       "  0.1176784411072731],\n",
       " 'accuracy': [0.596304178237915,\n",
       "  0.7562189102172852,\n",
       "  0.8678038120269775,\n",
       "  0.9317697286605835,\n",
       "  0.9765458703041077]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e928e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['statement'].to_numpy()\n",
    "test_seqs = tok.texts_to_sequences(test_text)\n",
    "test_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')\n",
    "\n",
    "test_labels = test['veracity'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2429eca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 36ms/step - loss: 1.9703 - accuracy: 0.5413\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295addeb",
   "metadata": {},
   "source": [
    "# Adding l2 regularization acctually slightly improved the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105abd4d",
   "metadata": {},
   "source": [
    "# Using Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bbfe4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "7/7 [==============================] - 12s 654ms/step - loss: 0.6947 - accuracy: 0.4732 - val_loss: 0.6933 - val_accuracy: 0.5312\n",
      "Epoch 2/7\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 0.6908 - accuracy: 0.5312 - val_loss: 0.6902 - val_accuracy: 0.5312\n",
      "Epoch 3/7\n",
      "7/7 [==============================] - 2s 217ms/step - loss: 0.6869 - accuracy: 0.5491 - val_loss: 0.6883 - val_accuracy: 0.5312\n",
      "Epoch 4/7\n",
      "7/7 [==============================] - 2s 220ms/step - loss: 0.7004 - accuracy: 0.4643 - val_loss: 0.6869 - val_accuracy: 0.6562\n",
      "Epoch 5/7\n",
      "7/7 [==============================] - 2s 222ms/step - loss: 0.6880 - accuracy: 0.5179 - val_loss: 0.6818 - val_accuracy: 0.5938\n",
      "Epoch 6/7\n",
      "7/7 [==============================] - 2s 222ms/step - loss: 0.6852 - accuracy: 0.5670 - val_loss: 0.6631 - val_accuracy: 0.6875\n",
      "Epoch 7/7\n",
      "2/7 [=======>......................] - ETA: 0s - loss: 0.6906 - accuracy: 0.5079WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 49 batches). You may need to use the repeat() function when building your dataset.\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.6906 - accuracy: 0.5079 - val_loss: 0.6637 - val_accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=7,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e378e56",
   "metadata": {},
   "source": [
    "# Using dropout er attained a respectable 68% accuracy on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0b5a2a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 85ms/step - loss: 0.6862 - accuracy: 0.5413\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d8b3f",
   "metadata": {},
   "source": [
    "# Tuning parameters in order to obtain better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59d0365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "7/7 [==============================] - 12s 674ms/step - loss: 0.6947 - accuracy: 0.4420 - val_loss: 0.6925 - val_accuracy: 0.5312\n",
      "Epoch 2/7\n",
      "7/7 [==============================] - 2s 216ms/step - loss: 0.6925 - accuracy: 0.5223 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
      "Epoch 3/7\n",
      "7/7 [==============================] - 2s 216ms/step - loss: 0.6908 - accuracy: 0.5268 - val_loss: 0.6923 - val_accuracy: 0.5312\n",
      "Epoch 4/7\n",
      "7/7 [==============================] - 2s 218ms/step - loss: 0.6972 - accuracy: 0.4509 - val_loss: 0.6945 - val_accuracy: 0.4688\n",
      "Epoch 5/7\n",
      "7/7 [==============================] - 2s 217ms/step - loss: 0.6918 - accuracy: 0.5312 - val_loss: 0.6888 - val_accuracy: 0.4688\n",
      "Epoch 6/7\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.6948 - accuracy: 0.5223 - val_loss: 0.6887 - val_accuracy: 0.5938\n",
      "Epoch 7/7\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 0.6836 - accuracy: 0.6190WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 49 batches). You may need to use the repeat() function when building your dataset.\n",
      "7/7 [==============================] - 1s 52ms/step - loss: 0.6836 - accuracy: 0.6190 - val_loss: 0.6940 - val_accuracy: 0.4375\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    keras.layers.GRU(70),\n",
    "    keras.layers.Dropout(rate=0.1),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    epochs=7,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4607c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 13s 663ms/step - loss: 0.6922 - accuracy: 0.5446 - val_loss: 0.6914 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 2s 218ms/step - loss: 0.6929 - accuracy: 0.5312 - val_loss: 0.6908 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.6904 - accuracy: 0.4955 - val_loss: 0.6911 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 1s 213ms/step - loss: 0.6989 - accuracy: 0.4464 - val_loss: 0.6947 - val_accuracy: 0.4062\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.6895 - accuracy: 0.5134 - val_loss: 0.6870 - val_accuracy: 0.5938\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 2s 245ms/step - loss: 0.7000 - accuracy: 0.5491 - val_loss: 0.6879 - val_accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Embedding( + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.3),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.1),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de83cf",
   "metadata": {},
   "source": [
    "#Try decreasing the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86719f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 13s 682ms/step - loss: 0.6944 - accuracy: 0.4732 - val_loss: 0.6906 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 2s 220ms/step - loss: 0.6921 - accuracy: 0.5179 - val_loss: 0.6887 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 2s 215ms/step - loss: 0.6879 - accuracy: 0.5268 - val_loss: 0.6878 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 2s 220ms/step - loss: 0.6976 - accuracy: 0.4911 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.6961 - accuracy: 0.5223 - val_loss: 0.6833 - val_accuracy: 0.5625\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 1s 214ms/step - loss: 0.6901 - accuracy: 0.5446 - val_loss: 0.6805 - val_accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(6000 + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model2.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0327d432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 17s 1s/step - loss: 0.6933 - accuracy: 0.5268 - val_loss: 0.6908 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 2s 300ms/step - loss: 0.6936 - accuracy: 0.5134 - val_loss: 0.6894 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.6885 - accuracy: 0.5536 - val_loss: 0.6891 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 0.6996 - accuracy: 0.4464 - val_loss: 0.6926 - val_accuracy: 0.4688\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 2s 302ms/step - loss: 0.6919 - accuracy: 0.5000 - val_loss: 0.6796 - val_accuracy: 0.6875\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 2s 292ms/step - loss: 0.6963 - accuracy: 0.5580 - val_loss: 0.6779 - val_accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model2.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e757f2a",
   "metadata": {},
   "source": [
    "# Determine if increasing the number of layers improves performance overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4078ece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 18s 975ms/step - loss: 0.6941 - accuracy: 0.4955 - val_loss: 0.6914 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 3s 393ms/step - loss: 0.6937 - accuracy: 0.5179 - val_loss: 0.6904 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 3s 391ms/step - loss: 0.6890 - accuracy: 0.5536 - val_loss: 0.6897 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 3s 396ms/step - loss: 0.6975 - accuracy: 0.4598 - val_loss: 0.6910 - val_accuracy: 0.5312\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 3s 381ms/step - loss: 0.6917 - accuracy: 0.5357 - val_loss: 0.6874 - val_accuracy: 0.5625\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 3s 384ms/step - loss: 0.6869 - accuracy: 0.5625 - val_loss: 0.6767 - val_accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model3.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e122cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 23s 1s/step - loss: 0.6949 - accuracy: 0.4554 - val_loss: 0.6916 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 4s 565ms/step - loss: 0.6926 - accuracy: 0.5312 - val_loss: 0.6908 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 4s 562ms/step - loss: 0.6868 - accuracy: 0.5714 - val_loss: 0.6928 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 4s 565ms/step - loss: 0.6931 - accuracy: 0.5134 - val_loss: 0.7029 - val_accuracy: 0.4688\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 4s 569ms/step - loss: 0.6943 - accuracy: 0.4955 - val_loss: 0.6890 - val_accuracy: 0.5938\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 4s 563ms/step - loss: 0.6970 - accuracy: 0.5714 - val_loss: 0.6866 - val_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "model4 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model4.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model4.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4d19d",
   "metadata": {},
   "source": [
    "## Model 2 has the most effective arhitecture so far, so we are going to improve on that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5c19b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 20s 1s/step - loss: 0.7993 - accuracy: 0.4821 - val_loss: 0.6913 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 4s 525ms/step - loss: 0.7344 - accuracy: 0.5491 - val_loss: 0.6909 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 4s 532ms/step - loss: 0.7856 - accuracy: 0.4911 - val_loss: 0.6919 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 4s 526ms/step - loss: 0.8291 - accuracy: 0.4732 - val_loss: 0.6915 - val_accuracy: 0.5312\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 4s 546ms/step - loss: 0.7825 - accuracy: 0.4955 - val_loss: 0.6907 - val_accuracy: 0.5312\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 4s 530ms/step - loss: 0.7559 - accuracy: 0.5223 - val_loss: 0.6948 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "model21 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.AlphaDropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model21.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model21.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48f5f9",
   "metadata": {},
   "source": [
    "# AlphaDropout does not add any improvement over simple Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020e823",
   "metadata": {},
   "source": [
    "## adding a more complex optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5dcf0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 18s 1s/step - loss: 0.7412 - accuracy: 0.5268 - val_loss: 0.6927 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 4s 573ms/step - loss: 0.8445 - accuracy: 0.4598 - val_loss: 0.6915 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 4s 561ms/step - loss: 0.8005 - accuracy: 0.4732 - val_loss: 0.6912 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 4s 554ms/step - loss: 0.7528 - accuracy: 0.5402 - val_loss: 0.6917 - val_accuracy: 0.5312\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 4s 571ms/step - loss: 0.7065 - accuracy: 0.5625 - val_loss: 0.6913 - val_accuracy: 0.5312\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 4s 577ms/step - loss: 0.7073 - accuracy: 0.5714 - val_loss: 0.6917 - val_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "model22 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.AlphaDropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.8, nesterov=True)\n",
    "model22.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model22.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fb4d3",
   "metadata": {},
   "source": [
    "# Adding Max Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1a133f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67df72c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 16s 823ms/step - loss: 0.7874 - accuracy: 0.4732 - val_loss: 0.6920 - val_accuracy: 0.5312\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 2s 226ms/step - loss: 0.7606 - accuracy: 0.4866 - val_loss: 0.6907 - val_accuracy: 0.5312\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 2s 228ms/step - loss: 0.7513 - accuracy: 0.5312 - val_loss: 0.6905 - val_accuracy: 0.5312\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 2s 227ms/step - loss: 0.8101 - accuracy: 0.4643 - val_loss: 0.6928 - val_accuracy: 0.4375\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 2s 230ms/step - loss: 0.7499 - accuracy: 0.5089 - val_loss: 0.6868 - val_accuracy: 0.5625\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 2s 230ms/step - loss: 0.7246 - accuracy: 0.5625 - val_loss: 0.6930 - val_accuracy: 0.4688\n"
     ]
    }
   ],
   "source": [
    "MaxNormGRU = partial(keras.layers.GRU,\n",
    "                       return_sequences=True,\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model24 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    MaxNormGRU(50),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    MaxNormGRU(50),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    MaxNormGRU(50),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(50),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model24.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model24.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81f42206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "44/44 [==============================] - 18s 279ms/step - loss: 0.6907 - accuracy: 0.5323\n",
      "Epoch 2/5\n",
      "44/44 [==============================] - 12s 277ms/step - loss: 0.5238 - accuracy: 0.7520\n",
      "Epoch 3/5\n",
      "44/44 [==============================] - 12s 279ms/step - loss: 0.1927 - accuracy: 0.9190\n",
      "Epoch 4/5\n",
      "44/44 [==============================] - 12s 275ms/step - loss: 0.0397 - accuracy: 0.9865\n",
      "Epoch 5/5\n",
      "44/44 [==============================] - 12s 278ms/step - loss: 0.0080 - accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model30 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model30.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model30.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c41e6c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 89ms/step - loss: 2.4807 - accuracy: 0.5248\n"
     ]
    }
   ],
   "source": [
    "results = model30.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7987255d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 38ms/step - loss: 2.1155 - accuracy: 0.5824\n"
     ]
    }
   ],
   "source": [
    "results2 = model30.evaluate(valid_seqs, valid_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d50e50e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 109ms/step - loss: 0.6827 - accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "restults3 = model3.evaluate(test_seqs, test_labels, batch_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60bc5f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 3s 69ms/step - loss: 0.6705 - accuracy: 0.5956\n"
     ]
    }
   ],
   "source": [
    "restults4 = model2.evaluate(valid_seqs, valid_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "96875954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "44/44 [==============================] - 27s 409ms/step - loss: 0.6916 - accuracy: 0.5167\n",
      "Epoch 2/5\n",
      "44/44 [==============================] - 18s 414ms/step - loss: 0.5198 - accuracy: 0.7392\n",
      "Epoch 3/5\n",
      "44/44 [==============================] - 18s 417ms/step - loss: 0.2429 - accuracy: 0.9055\n",
      "Epoch 4/5\n",
      "44/44 [==============================] - 18s 416ms/step - loss: 0.0648 - accuracy: 0.9780\n",
      "Epoch 5/5\n",
      "44/44 [==============================] - 18s 419ms/step - loss: 0.0245 - accuracy: 0.9908\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model33 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True, kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model33.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model33.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fb47db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 4s 73ms/step - loss: 2.2694 - accuracy: 0.5314\n"
     ]
    }
   ],
   "source": [
    "restults5 = model33.evaluate(test_seqs, test_labels, batch_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0da318fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 165ms/step - loss: 0.6837 - accuracy: 0.5385\n"
     ]
    }
   ],
   "source": [
    "restults3 = model3.evaluate(valid_seqs, valid_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf389cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
