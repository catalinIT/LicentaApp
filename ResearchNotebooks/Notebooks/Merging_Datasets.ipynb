{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30324a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    %pip install -q -U tensorflow-addons\n",
    "    %pip install -q -U transformers\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5cf752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Tim Kaine doesn’t want a border at all. He wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The deficit ... is coming down, and it’s comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Migrant mother and ‘crying girl’ on Time cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Fact: Over 90,000 kids were detained under Ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>\"$1 billion—that’s how much Bruce Rauner has w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    veracity                                          statement\n",
       "3          0  \"Tim Kaine doesn’t want a border at all. He wa...\n",
       "5          0  \"The deficit ... is coming down, and it’s comi...\n",
       "20         1  \"Migrant mother and ‘crying girl’ on Time cove...\n",
       "23         0  \"Fact: Over 90,000 kids were detained under Ob...\n",
       "24         0  \"$1 billion—that’s how much Bruce Rauner has w..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv to df\n",
    "path = os.path.abspath('politifact_balanced_data.csv')\n",
    "df_political = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# specify rows of importance\n",
    "df_political = df_political[[\"veracity\", \"statement\"]]\n",
    "\n",
    "# See sample of data\n",
    "df_political.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399b0af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No sir I did not. I absolutely did not. No sir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>... and she approached me, and at that time th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>No sir I was not, not at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>He had told me that he had had a dream that, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>And he told me that, ammm â€¦ he was trying to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veracity                                          statement\n",
       "0         0  No sir I did not. I absolutely did not. No sir...\n",
       "1         0  ... and she approached me, and at that time th...\n",
       "2         0                      No sir I was not, not at all.\n",
       "3         0  He had told me that he had had a dream that, a...\n",
       "4         0  And he told me that, ammm â€¦ he was trying to..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath(r'C:\\Users\\catat\\OneDrive\\Desktop\\DataForBachelorThesis\\Real-lifeDeceptionDetection2016\\Transcription\\Truthful\\transcripts.csv')\n",
    "df_small = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# specify rows of importance\n",
    "df_small = df_small[[\"veracity\", \"statement\"]]\n",
    "\n",
    "# See sample of data\n",
    "df_small.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40a74a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>There is a great deal of truth to the anti-vax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Jenny mccarthy is a learned doctor who deserve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Driving doesn\\t really require any practice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Drinking and driving is a winning and safe com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Good hygiene isn\\t really important or attract...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veracity                                          statement\n",
       "0         0  There is a great deal of truth to the anti-vax...\n",
       "1         0  Jenny mccarthy is a learned doctor who deserve...\n",
       "2         0       Driving doesn\\t really require any practice.\n",
       "3         0  Drinking and driving is a winning and safe com...\n",
       "4         0  Good hygiene isn\\t really important or attract..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath('sevenDataset_cleaned.csv')\n",
    "df_7t = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# specify rows of importance\n",
    "df_7t = df_7t[[\"veracity\", \"statement\"]]\n",
    "\n",
    "# See sample of data\n",
    "df_7t.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faf5eb",
   "metadata": {},
   "source": [
    "# Check the average lenght of a statement from each df  before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e233575",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_avg_l = df_political['statement'].apply(len).mean()\n",
    "small_avg_l = df_small['statement'].apply(len).mean()\n",
    "sevenT_avg_l = df_7t['statement'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec9be17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.79260969976906\n",
      "337.92561983471074\n",
      "39.43523447401775\n"
     ]
    }
   ],
   "source": [
    "print(political_avg_l)\n",
    "print(small_avg_l)\n",
    "print(sevenT_avg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17716f58",
   "metadata": {},
   "source": [
    "# For the smaller Trascripts dataset, the average lenght is way bigger. If we get poor results, we may drop this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8298305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united = pd.concat([df_political,df_small])\n",
    "df_united = pd.concat([df_united, df_7t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32836358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6231 entries, 3 to 3944\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   6231 non-null   int64 \n",
      " 1   statement  6231 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 146.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_united.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20473546",
   "metadata": {},
   "source": [
    "# Split the dataframe into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9828521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_united.sample(frac=0.7,random_state=200) #random state is a seed value\n",
    "df_validation = df_united.drop(train.index)\n",
    "validation=df_validation.sample(frac=0.6,random_state=200)\n",
    "test = df_validation.drop(validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3433f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4985 entries, 1134 to 3804\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   4985 non-null   int64 \n",
      " 1   statement  4985 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 116.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 551 entries, 10072 to 3134\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   551 non-null    int64 \n",
      " 1   statement  551 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 354 entries, 123 to 3932\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   354 non-null    int64 \n",
      " 1   statement  354 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "validation.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b42ee71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_l = train['statement'].apply(len).mean()\n",
    "valid_avg_l = validation['statement'].apply(len).mean()\n",
    "test_avg_l = test['statement'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ce4da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.02908726178535\n",
      "63.401088929219604\n",
      "62.090395480225986\n"
     ]
    }
   ],
   "source": [
    "#compute the average lenght from each dataset to ensure the random distribution of the data\n",
    "print(train_avg_l)\n",
    "print(valid_avg_l)\n",
    "print(test_avg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f25f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_text = train['statement'].to_numpy()\n",
    "tok = Tokenizer(oov_token='<unk>')\n",
    "tok.fit_on_texts(train_text)\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "train_labels = train['veracity'].to_numpy().flatten()\n",
    "\n",
    "valid_text = validation['statement'].to_numpy()\n",
    "valid_seqs = tok.texts_to_sequences(valid_text)\n",
    "valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')\n",
    "\n",
    "valid_labels = validation['veracity'].to_numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT TO TF DATASETS\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 7\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# PREFETCH\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d0f69",
   "metadata": {},
   "source": [
    "# Designing a simple model without validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cce5b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 46s 259ms/step - loss: 0.6874 - accuracy: 0.5384\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 48s 308ms/step - loss: 0.4861 - accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 48s 305ms/step - loss: 0.2426 - accuracy: 0.9031\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 46s 298ms/step - loss: 0.1720 - accuracy: 0.9286\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 48s 305ms/step - loss: 0.1473 - accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55c67f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6873899698257446,\n",
       "  0.4861297905445099,\n",
       "  0.24258652329444885,\n",
       "  0.17198337614536285,\n",
       "  0.14731137454509735],\n",
       " 'accuracy': [0.538415253162384,\n",
       "  0.7650952935218811,\n",
       "  0.9031093120574951,\n",
       "  0.9285857677459717,\n",
       "  0.9422267079353333]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e932e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['statement'].to_numpy()\n",
    "test_seqs = tok.texts_to_sequences(test_text)\n",
    "test_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')\n",
    "\n",
    "test_labels = test['veracity'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b60c5973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 19ms/step - loss: 1.2466 - accuracy: 0.5904\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69795a7",
   "metadata": {},
   "source": [
    "# Designing a simple model with validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "178250f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 9s 647ms/step - loss: 0.6943 - accuracy: 0.4509 - val_loss: 0.6908 - val_accuracy: 0.5938\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 2s 273ms/step - loss: 0.6880 - accuracy: 0.5536 - val_loss: 0.6808 - val_accuracy: 0.5938\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 2s 268ms/step - loss: 0.7019 - accuracy: 0.4821 - val_loss: 0.6828 - val_accuracy: 0.5938\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 0.6899 - accuracy: 0.5446 - val_loss: 0.6860 - val_accuracy: 0.5938\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 2s 260ms/step - loss: 0.6935 - accuracy: 0.4643 - val_loss: 0.6912 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcae7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 29ms/step - loss: 0.6926 - accuracy: 0.5311\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf836fc",
   "metadata": {},
   "source": [
    "# Fine tuning the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91bd5d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7/7 [==============================] - 19s 1s/step - loss: 0.6942 - accuracy: 0.5134 - val_loss: 0.6860 - val_accuracy: 0.5938\n",
      "Epoch 2/6\n",
      "7/7 [==============================] - 4s 556ms/step - loss: 0.6928 - accuracy: 0.5580 - val_loss: 0.6804 - val_accuracy: 0.5938\n",
      "Epoch 3/6\n",
      "7/7 [==============================] - 4s 574ms/step - loss: 0.6965 - accuracy: 0.4911 - val_loss: 0.6908 - val_accuracy: 0.5938\n",
      "Epoch 4/6\n",
      "7/7 [==============================] - 4s 563ms/step - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6876 - val_accuracy: 0.7812\n",
      "Epoch 5/6\n",
      "7/7 [==============================] - 4s 557ms/step - loss: 0.6941 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.4375\n",
      "Epoch 6/6\n",
      "7/7 [==============================] - 4s 568ms/step - loss: 0.6960 - accuracy: 0.5268 - val_loss: 0.6949 - val_accuracy: 0.4062\n"
     ]
    }
   ],
   "source": [
    "#model that had the best results on the validation set\n",
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model2.fit(\n",
    "    train_ds,\n",
    "    epochs=6,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc9b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 22s 1s/step - loss: 0.6937 - accuracy: 0.5312 - val_loss: 0.6870 - val_accuracy: 0.5938\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 6s 865ms/step - loss: 0.7083 - accuracy: 0.5446 - val_loss: 0.6814 - val_accuracy: 0.5938\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 6s 865ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6883 - val_accuracy: 0.5938\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 6s 861ms/step - loss: 0.6927 - accuracy: 0.5268 - val_loss: 0.6903 - val_accuracy: 0.6250\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 6s 872ms/step - loss: 0.6929 - accuracy: 0.4955 - val_loss: 0.6995 - val_accuracy: 0.4062\n"
     ]
    }
   ],
   "source": [
    "#model that had the best results on the validation set\n",
    "model4 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model4.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model4.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9713c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 20s 1s/step - loss: 0.6930 - accuracy: 0.5223 - val_loss: 0.6854 - val_accuracy: 0.5938\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 5s 757ms/step - loss: 0.6918 - accuracy: 0.5536 - val_loss: 0.6820 - val_accuracy: 0.5938\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 5s 746ms/step - loss: 0.6974 - accuracy: 0.4866 - val_loss: 0.6919 - val_accuracy: 0.5625\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 5s 747ms/step - loss: 0.6926 - accuracy: 0.5312 - val_loss: 0.6886 - val_accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "#model that had the best results on the validation set\n",
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model3.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model3.fit(\n",
    "    train_ds,\n",
    "    epochs=4,\n",
    "    validation_data=valid_ds, validation_steps=1, steps_per_epoch=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5244534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 88ms/step - loss: 0.6934 - accuracy: 0.4859\n"
     ]
    }
   ],
   "source": [
    "results2 = model2.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33b43320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 87ms/step - loss: 0.6916 - accuracy: 0.5113\n"
     ]
    }
   ],
   "source": [
    "results3 = model3.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31793b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 111ms/step - loss: 0.6938 - accuracy: 0.4915\n"
     ]
    }
   ],
   "source": [
    "results4 = model4.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60413602",
   "metadata": {},
   "source": [
    "# Overall poor results. Applying Max norm is in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d21b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 176s 1s/step - loss: 0.6884 - accuracy: 0.5430\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 166s 1s/step - loss: 0.4942 - accuracy: 0.7551\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 165s 1s/step - loss: 0.2368 - accuracy: 0.9029\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 178s 1s/step - loss: 0.1652 - accuracy: 0.9378\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 180s 1s/step - loss: 0.1572 - accuracy: 0.9386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c403930bb0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model33 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True, kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, kernel_constraint=keras.constraints.max_norm(1.)),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model33.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model33.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf683a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 6s 162ms/step - loss: 1.4239 - accuracy: 0.5263\n"
     ]
    }
   ],
   "source": [
    "restults3 = model33.evaluate(valid_seqs, valid_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "070f1d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 104ms/step - loss: 1.4354 - accuracy: 0.5537\n"
     ]
    }
   ],
   "source": [
    "restults3 = model33.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed2970",
   "metadata": {},
   "source": [
    "# Train the models without the long outliers from the small df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f03ce67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united_no_small = pd.concat([df_political,df_7t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "624a7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_united_no_small.sample(frac=0.7,random_state=200) #random state is a seed value\n",
    "df_validation = df_united_no_small.drop(train.index)\n",
    "validation=df_validation.sample(frac=0.6,random_state=200)\n",
    "test = df_validation.drop(validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "946f3b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4277 entries, 3905 to 2922\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   4277 non-null   int64 \n",
      " 1   statement  4277 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 100.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 894 entries, 2544 to 1468\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   894 non-null    int64 \n",
      " 1   statement  894 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 21.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 561 entries, 100 to 3919\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   veracity   561 non-null    int64 \n",
      " 1   statement  561 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 13.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "validation.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b38f68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_l = train['statement'].apply(len).mean()\n",
    "valid_avg_l = validation['statement'].apply(len).mean()\n",
    "test_avg_l = test['statement'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bb866e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.805237315875615\n",
      "62.93400447427293\n",
      "58.105169340463455\n"
     ]
    }
   ],
   "source": [
    "#compute the average lenght from each dataset to ensure the random distribution of the data\n",
    "print(train_avg_l)\n",
    "print(valid_avg_l)\n",
    "print(test_avg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f0994ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_text = train['statement'].to_numpy()\n",
    "tok = Tokenizer(oov_token='<unk>')\n",
    "tok.fit_on_texts(train_text)\n",
    "tok.word_index['<pad>'] = 0\n",
    "tok.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tok.texts_to_sequences(train_text)\n",
    "train_seqs = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "train_labels = train['veracity'].to_numpy().flatten()\n",
    "\n",
    "valid_text = validation['statement'].to_numpy()\n",
    "valid_seqs = tok.texts_to_sequences(valid_text)\n",
    "valid_seqs = tf.keras.preprocessing.sequence.pad_sequences(valid_seqs, padding='post')\n",
    "\n",
    "valid_labels = validation['veracity'].to_numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT TO TF DATASETS\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_seqs,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_seqs,valid_labels))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 7\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# PREFETCH\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6008911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "134/134 [==============================] - 37s 239ms/step - loss: 0.6895 - accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "134/134 [==============================] - 32s 241ms/step - loss: 0.5067 - accuracy: 0.7533\n",
      "Epoch 3/5\n",
      "134/134 [==============================] - 32s 236ms/step - loss: 0.2678 - accuracy: 0.8896\n",
      "Epoch 4/5\n",
      "134/134 [==============================] - 31s 234ms/step - loss: 0.1841 - accuracy: 0.9231\n",
      "Epoch 5/5\n",
      "134/134 [==============================] - 32s 236ms/step - loss: 0.1487 - accuracy: 0.9418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c381ff1610>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 128\n",
    "num_oov_buckets = 1000\n",
    "vocab_size=10000\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02373b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['statement'].to_numpy()\n",
    "test_seqs = tok.texts_to_sequences(test_text)\n",
    "test_seqs = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')\n",
    "\n",
    "test_labels = test['veracity'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8865785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 92ms/step - loss: 1.3535 - accuracy: 0.5223\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_seqs, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d3be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
